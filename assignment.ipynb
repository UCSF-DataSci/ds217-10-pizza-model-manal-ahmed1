{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4dbdea",
   "metadata": {},
   "source": [
    "# Assignment 10: Modeling Fundamentals\n",
    "\n",
    "Complete the following three questions to demonstrate your understanding of statistical modeling, machine learning, and gradient boosting.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddde37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ XGBoost import failed: XGBoostError\n",
      "  Using sklearn's GradientBoostingRegressor as alternative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import os\n",
    "\n",
    "# Try to import XGBoost, fall back to sklearn's GradientBoosting if needed\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"✓ XGBoost loaded successfully\")\n",
    "except (ImportError, Exception) as e:\n",
    "    print(f\"⚠ XGBoost import failed: {type(e).__name__}\")\n",
    "    print(\"  Using sklearn's GradientBoostingRegressor as alternative\")\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6047b91",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2f4a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20640 housing records\n",
      "\n",
      "Feature names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n",
      "First few rows:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  house_value  \n",
      "0    -122.23        4.526  \n",
      "1    -122.22        3.585  \n",
      "2    -122.24        3.521  \n",
      "3    -122.25        3.413  \n",
      "4    -122.25        3.422  \n",
      "\n",
      "Summary statistics:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   house_value  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558  \n",
      "std       10.386050      2.135952      2.003532      1.153956  \n",
      "min        0.692308     32.540000   -124.350000      0.149990  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010  \n"
     ]
    }
   ],
   "source": [
    "# Load California Housing dataset from scikit-learn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch the dataset\n",
    "housing_data = fetch_california_housing(as_frame=True)\n",
    "df = housing_data.frame\n",
    "\n",
    "# Rename target for clarity\n",
    "df = df.rename(columns={\"MedHouseVal\": \"house_value\"})\n",
    "\n",
    "print(f\"Loaded {len(df)} housing records\")\n",
    "print(\"\\nFeature names:\", housing_data.feature_names)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e07a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Statistical Modeling with statsmodels\n",
    "\n",
    "**Note:** This question focuses on statistical modeling for inference - understanding relationships between variables. We'll use a subset of features (`MedInc`, `AveBedrms`, `Population`) to focus on interpretability and statistical significance rather than maximizing prediction accuracy. The `statsmodels` library provides detailed statistical information (p-values, confidence intervals, AIC) that helps us understand *why* variables are related.\n",
    "\n",
    "**Why a subset of features?** In statistical modeling, we often use fewer features to maintain interpretability and focus on understanding relationships. This contrasts with machine learning (Question 2), where we use all available features to maximize prediction accuracy.\n",
    "\n",
    "**Objective:** Fit linear regression models using `statsmodels`, extract statistical information, and compare models with and without interaction terms.\n",
    "\n",
    "### Part 1.1: Fit the Model\n",
    "\n",
    "Fit a linear regression model predicting `house_value` from `MedInc`, `AveBedrms`, and `Population` using the formula API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cc421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit a linear regression model using statsmodels formula API\n",
    "# Hint: Use smf.ols() with the formula 'house_value ~ MedInc + AveBedrms + Population'\n",
    "# Don't forget to call .fit() on the model\n",
    "\n",
    "model = smf.ols('house_value ~ MedInc + AveBedrms + Population', data=df)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad3971",
   "metadata": {},
   "source": [
    "### Part 1.2: Extract Model Summary\n",
    "\n",
    "Print the model summary and save key statistics to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0104e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            house_value   R-squared:                       0.474\n",
      "Model:                            OLS   Adj. R-squared:                  0.474\n",
      "Method:                 Least Squares   F-statistic:                     6205.\n",
      "Date:                Thu, 04 Dec 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:47:57   Log-Likelihood:                -25607.\n",
      "No. Observations:               20640   AIC:                         5.122e+04\n",
      "Df Residuals:                   20636   BIC:                         5.125e+04\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5084      0.021     24.089      0.000       0.467       0.550\n",
      "MedInc         0.4178      0.003    136.014      0.000       0.412       0.424\n",
      "AveBedrms     -0.0144      0.012     -1.165      0.244      -0.039       0.010\n",
      "Population  -2.89e-05   5.15e-06     -5.608      0.000    -3.9e-05   -1.88e-05\n",
      "==============================================================================\n",
      "Omnibus:                     4188.894   Durbin-Watson:                   0.659\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9119.557\n",
      "Skew:                           1.177   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.249   Cond. No.                     7.23e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.23e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "=== Coefficient Significance (p-values) ===\n",
      "Intercept: p-value = 0.0000 (SIGNIFICANT)\n",
      "MedInc: p-value = 0.0000 (SIGNIFICANT)\n",
      "AveBedrms: p-value = 0.2441 (not significant)\n",
      "Population: p-value = 0.0000 (SIGNIFICANT)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Print the model summary\n",
    "# Use: results.summary()\n",
    "\n",
    "print(results.summary())\n",
    "# Your code here\n",
    "\n",
    "# TODO: Extract p-values for coefficients\n",
    "# Use: results.pvalues to get p-values for each coefficient\n",
    "# Print which coefficients are statistically significant (p < 0.05)\n",
    "\n",
    "pvalues = results.pvalues  # Replace None with your code\n",
    "print(\"\\n=== Coefficient Significance (p-values) ===\")\n",
    "# Your code here to print p-values and identify significant coefficients\n",
    "\n",
    "# TODO: Save key statistics to output file\n",
    "# Extract: R-squared, number of observations, and AIC (Akaike Information Criterion)\n",
    "# Format: \"R-squared: X.XXXX\\nObservations: XXXX\\nAIC: XXXXX.XX\"\n",
    "for coef, p in pvalues.items():\n",
    "    significance = \"SIGNIFICANT\" if p < 0.05 else \"not significant\"\n",
    "    print(f\"{coef}: p-value = {p:.4f} ({significance})\")\n",
    "\n",
    "with open(\"output/q1_model_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"R-squared: {results.rsquared:.4f}\\n\")\n",
    "    f.write(f\"Observations: {int(results.nobs)}\\n\")\n",
    "    f.write(f\"AIC: {results.aic:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355071a0",
   "metadata": {},
   "source": [
    "### Part 1.3: Make Predictions\n",
    "\n",
    "Make predictions for all houses and save to CSV.\n",
    "\n",
    "**Note:** In statistical modeling, we often make predictions on the full dataset to understand model fit. This differs from machine learning (Question 2), where we use train/test splits to evaluate generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8530a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 20640 predictions to output/q1_statistical_model.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make predictions using the fitted model\n",
    "# Hint: Use results.predict() with a DataFrame containing the features used in the model\n",
    "# The features are: MedInc, AveBedrms, Population\n",
    "# Save predictions along with actual values to CSV\n",
    "\n",
    "predictions = results.predict(df[[\"MedInc\", \"AveBedrms\", \"Population\"]])\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": df[\"house_value\"],\n",
    "    \"predicted_value\": predictions,\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "pred_df.to_csv(\"output/q1_statistical_model.csv\", index=False)\n",
    "print(f\"\\nSaved {len(pred_df)} predictions to output/q1_statistical_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053bf61",
   "metadata": {},
   "source": [
    "### Part 1.4: Model with Interaction Term\n",
    "\n",
    "Now let's fit a model with an interaction term. An **interaction term** allows the effect of one variable to depend on the value of another variable. For example, the effect of income (`MedInc`) on house value might depend on the number of bedrooms (`AveBedrms`). In the formula API, we use `*` to include both main effects and their interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c296d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model with Interaction Term ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Fit a model with an interaction term between MedInc and AveBedrms\n",
    "# Hint: Use formula 'house_value ~ MedInc + AveBedrms + Population + MedInc:AveBedrms'\n",
    "# Or use 'house_value ~ MedInc * AveBedrms + Population' (the * includes both main effects and interaction)\n",
    "\n",
    "model_interaction = smf.ols(\n",
    "    'house_value ~ MedInc * AveBedrms + Population',\n",
    "    data=df)  # Replace None with your code\n",
    "results_interaction =  model_interaction.fit()  # Replace None with your code\n",
    "\n",
    "print(\"\\n=== Model with Interaction Term ===\")\n",
    "# Your code here to print the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83bc2c1",
   "metadata": {},
   "source": [
    "### Part 1.5: Compare Models\n",
    "\n",
    "Compare the two models using AIC (Akaike Information Criterion). Lower AIC indicates a better model (accounting for model complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10941b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "Simple model AIC: 51221.28\n",
      "Interaction model AIC: 51220.38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Compare the two models using AIC\n",
    "# Extract AIC from both models: results.aic and results_interaction.aic\n",
    "# Determine which model is better (lower AIC is better)\n",
    "\n",
    "aic_simple = results.aic  # Replace None with your code\n",
    "aic_interaction = results_interaction.aic  # Replace None with your code\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Simple model AIC: {aic_simple:.2f}\")\n",
    "print(f\"Interaction model AIC: {aic_interaction:.2f}\")\n",
    "# Your code here to determine and print which model is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744c660",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Machine Learning with scikit-learn\n",
    "\n",
    "**Note:** While Question 1 focused on statistical inference (understanding relationships and testing hypotheses), Question 2 focuses on machine learning for prediction. We'll use all available features to maximize prediction accuracy rather than focusing on interpretability.\n",
    "\n",
    "**Objective:** Fit and compare linear regression and random forest models using `scikit-learn`.\n",
    "\n",
    "### Part 2.1: Prepare Data\n",
    "\n",
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8c6591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 16512 samples\n",
      "Test set: 4128 samples\n"
     ]
    }
   ],
   "source": [
    "# TODO: Prepare features and target\n",
    "# Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "# Target: 'house_value'\n",
    "\n",
    "feature_cols = [\n",
    "    \"MedInc\",\n",
    "    \"HouseAge\",\n",
    "    \"AveRooms\",\n",
    "    \"AveBedrms\",\n",
    "    \"Population\",\n",
    "    \"AveOccup\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]\n",
    "X = df[feature_cols] # Replace None with your code\n",
    "y = df['house_value'].values # Replace None with your code\n",
    "\n",
    "# TODO: Split into train and test sets (80/20 split, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # Replace None with your code\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c89cb",
   "metadata": {},
   "source": [
    "### Part 2.2: Fit Linear Regression\n",
    "\n",
    "Fit a linear regression model and evaluate it on both training and test sets. Comparing train and test performance helps us detect overfitting - if the model performs much better on training data than test data, it's likely overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e5aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear Regression Results ===\n",
      "Training - R²: 0.6126, RMSE: 0.72\n",
      "Test - R²: 0.5758, RMSE: 0.75\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit a LinearRegression model\n",
    "lr_model = LinearRegression() # Replace None with your code\n",
    "lr_model.fit(X_train, y_train) # Your code here\n",
    "\n",
    "# TODO: Make predictions on both training and test sets\n",
    "lr_train_pred = lr_model.predict(X_train) # Replace None with your code\n",
    "lr_test_pred = lr_model.predict(X_test)  # Replace None with your code\n",
    "\n",
    "# Calculate metrics on both sets\n",
    "lr_train_r2 = r2_score(y_train, lr_train_pred)\n",
    "lr_test_r2 = r2_score(y_test, lr_test_pred)\n",
    "lr_train_rmse = np.sqrt(mean_squared_error(y_train, lr_train_pred))\n",
    "lr_test_rmse = np.sqrt(mean_squared_error(y_test, lr_test_pred))\n",
    "\n",
    "print(\"=== Linear Regression Results ===\")\n",
    "print(f\"Training - R²: {lr_train_r2:.4f}, RMSE: {lr_train_rmse:.2f}\")\n",
    "print(f\"Test - R²: {lr_test_r2:.4f}, RMSE: {lr_test_rmse:.2f}\")\n",
    "\n",
    "# Store test predictions for later use\n",
    "lr_pred = lr_test_pred\n",
    "lr_r2 = lr_test_r2\n",
    "lr_rmse = lr_test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036c8bd",
   "metadata": {},
   "source": [
    "### Part 2.3: Fit Random Forest\n",
    "\n",
    "Fit a random forest model and evaluate it on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8f4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Results ===\n",
      "Training - R²: 0.8050, RMSE: 0.51\n",
      "Test - R²: 0.7389, RMSE: 0.58\n",
      "\n",
      "== Feature Importance ===\n",
      "      feature  importance\n",
      "0      MedInc    0.644841\n",
      "5    AveOccup    0.140570\n",
      "6    Latitude    0.062540\n",
      "7   Longitude    0.060256\n",
      "1    HouseAge    0.046583\n",
      "2    AveRooms    0.025055\n",
      "4  Population    0.010428\n",
      "3   AveBedrms    0.009727\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit a RandomForestRegressor model\n",
    "# Use: n_estimators=50, max_depth=8, random_state=42\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators = 50,\n",
    "    max_depth = 8,\n",
    "    random_state = 42\n",
    ")  \n",
    "rf_model.fit(X_train, y_train) # Replace None with your code\n",
    "\n",
    "# TODO: Make predictions on both training and test sets\n",
    "rf_train_pred = rf_model.predict(X_train)  # Replace None with your code\n",
    "rf_test_pred = rf_model.predict(X_test) # Replace None with your code\n",
    "\n",
    "# Calculate metrics on both sets\n",
    "rf_train_r2 = r2_score(y_train, rf_train_pred)\n",
    "rf_test_r2 = r2_score(y_test, rf_test_pred)\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "\n",
    "print(\"=== Random Forest Results ===\")\n",
    "print(f\"Training - R²: {rf_train_r2:.4f}, RMSE: {rf_train_rmse:.2f}\")\n",
    "print(f\"Test - R²: {rf_test_r2:.4f}, RMSE: {rf_test_rmse:.2f}\")\n",
    "\n",
    "# Store test predictions and metrics for later use\n",
    "rf_pred = rf_test_pred\n",
    "rf_r2 = rf_test_r2\n",
    "rf_rmse = rf_test_rmse\n",
    "\n",
    "# TODO: Extract feature importance for later comparison\n",
    "# Use: rf_model.feature_importances_\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending = False)\n",
    "\n",
    "print(\"\\n== Feature Importance ===\")\n",
    "print(rf_feature_importance)\n",
    "    \n",
    "  # Replace None with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07dec4",
   "metadata": {},
   "source": [
    "### Part 2.4: Save Predictions and Comparison\n",
    "\n",
    "Save predictions and model comparison to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610aef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to output/q2_ml_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save predictions to CSV\n",
    "# Include: actual_value, lr_predicted_value, rf_predicted_value\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": y_test,\n",
    "    \"lr_predicted_value\": lr_pred,\n",
    "    \"rf_predicted_value\": rf_pred,\n",
    "})\n",
    "\n",
    "pred_df.to_csv(\"output/q2_ml_predictions.csv\", index=False)\n",
    "print(f\"\\nSaved predictions to output/q2_ml_predictions.csv\")\n",
    "\n",
    "# TODO: Save model comparison to text file\n",
    "# Include both train and test metrics\n",
    "# Format: \"Linear Regression - Train R²: X.XXXX, Test R²: X.XXXX, Test RMSE: XX.XX\\nRandom Forest - Train R²: X.XXXX, Test R²: X.XXXX, Test RMSE: XX.XX\"\n",
    "\n",
    "with open(\"output/q2_model_comparison.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        f\"Linear Regression - Train R²: {lr_train_r2:.4f}, Test R²: {lr_test_r2:.4f}, Test RMSE: {lr_test_rmse:.2f}\\n\"\n",
    "        f\"Random Forest - Train R²: {rf_train_r2:.4f}, Test R²: {rf_test_r2:.4f}, Test RMSE: {rf_test_rmse:.2f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7ae27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Gradient Boosting with XGBoost\n",
    "\n",
    "**Note:** Question 3 introduces gradient boosting, an advanced machine learning technique that often achieves the best performance on tabular data. XGBoost builds models sequentially, with each new model learning from the mistakes of previous ones.\n",
    "\n",
    "**Objective:** Fit an XGBoost model and extract feature importance.\n",
    "\n",
    "### Part 3.1: Fit XGBoost Model\n",
    "\n",
    "Fit an XGBoost regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c839d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting (sklearn) - R²: 0.7922, RMSE: 0.52\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit an XGBRegressor or GradientBoosting model\n",
    "# Use: n_estimators=100, max_depth=3, learning_rate=0.15, random_state=42\n",
    "if XGBOOST_AVAILABLE:\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=3, \n",
    "        learning_rate=0.15, \n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    # Fall back to sklearn's GradientBoostingRegressor\n",
    "    xgb_model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions on test set\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "\n",
    "model_type = \"XGBoost\" if XGBOOST_AVAILABLE else \"Gradient Boosting (sklearn)\"\n",
    "print(f\"{model_type} - R²: {xgb_r2:.4f}, RMSE: {xgb_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c8744",
   "metadata": {},
   "source": [
    "### Part 3.2: Extract and Compare Feature Importance\n",
    "\n",
    "Extract feature importance from XGBoost and compare it with Random Forest from Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458210b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gradient Boosting Feature Importance ===\n",
      "      feature  xgb_importance\n",
      "0      MedInc        0.592435\n",
      "5    AveOccup        0.127074\n",
      "7   Longitude        0.113308\n",
      "6    Latitude        0.096346\n",
      "1    HouseAge        0.035586\n",
      "2    AveRooms        0.024784\n",
      "3   AveBedrms        0.006666\n",
      "4  Population        0.003802\n",
      "\n",
      "=== Feature Importance Comparison ===\n",
      "      feature  random_forest  gradient_boosting\n",
      "0      MedInc       0.644841           0.592435\n",
      "5    AveOccup       0.025055           0.127074\n",
      "7   Longitude       0.009727           0.113308\n",
      "6    Latitude       0.010428           0.096346\n",
      "1    HouseAge       0.140570           0.035586\n",
      "2    AveRooms       0.062540           0.024784\n",
      "3   AveBedrms       0.060256           0.006666\n",
      "4  Population       0.046583           0.003802\n"
     ]
    }
   ],
   "source": [
    "# TODO: Extract feature importance from XGBoost/GradientBoosting\n",
    "# Use: xgb_model.feature_importances_\n",
    "xgb_feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create DataFrame for XGBoost importance\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"xgb_importance\": xgb_feature_importance,\n",
    "}).sort_values(\"xgb_importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Gradient Boosting Feature Importance ===\")\n",
    "print(xgb_importance_df)\n",
    "\n",
    "# TODO: Compare with Random Forest feature importance\n",
    "# Create a comparison DataFrame with both models' feature importance\n",
    "# Sort by Gradient Boosting importance for display\n",
    "\n",
    "importance_comparison = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"random_forest\": rf_feature_importance[\"importance\"].values,\n",
    "    \"gradient_boosting\": xgb_feature_importance,\n",
    "}).sort_values(\"gradient_boosting\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Feature Importance Comparison ===\")\n",
    "print(importance_comparison)\n",
    "\n",
    "# TODO: Save feature importance to text file\n",
    "# Format: \"feature_name: X.XXXX\" (one per line, sorted by importance)\n",
    "\n",
    "with open(\"output/q3_feature_importance.txt\", \"w\") as f:\n",
    "    f.write(\"=== Gradient Boosting Feature Importance ===\\n\")\n",
    "    for idx, row in xgb_importance_df.iterrows():\n",
    "        f.write(f\"{row['feature']}: {row['xgb_importance']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3349430",
   "metadata": {},
   "source": [
    "### Part 3.3: Save Predictions\n",
    "\n",
    "Save XGBoost predictions to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8a4dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to output/q3_xgboost_model.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save predictions to CSV\n",
    "# Include: actual_value, xgb_predicted_value\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": y_test,\n",
    "    \"xgb_predicted_value\": xgb_pred,\n",
    "})\n",
    "\n",
    "pred_df.to_csv(\"output/q3_xgboost_model.csv\", index=False)\n",
    "print(f\"\\nSaved predictions to output/q3_xgboost_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9bcb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify you've created all required output files:\n",
    "\n",
    "- [ ] `output/q1_statistical_model.csv`\n",
    "- [ ] `output/q1_model_summary.txt`\n",
    "- [ ] `output/q2_ml_predictions.csv`\n",
    "- [ ] `output/q2_model_comparison.txt`\n",
    "- [ ] `output/q3_xgboost_model.csv`\n",
    "- [ ] `output/q3_feature_importance.txt`\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
